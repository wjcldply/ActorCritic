{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fd834f1b-51c4-4910-a4d6-3b212e1a2a5a",
      "metadata": {
        "id": "fd834f1b-51c4-4910-a4d6-3b212e1a2a5a"
      },
      "source": [
        "## Actor Critic\n",
        "---\n",
        "\n",
        "Policy Gradient의 Actor Critic 실습자료 입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cXDgFfdVBsNQ",
      "metadata": {
        "id": "cXDgFfdVBsNQ"
      },
      "source": [
        "import 및 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "cf4GE56I_OlT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf4GE56I_OlT",
        "outputId": "c6b5877a-7028-423e-ed0f-e78b02977305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "faSauiLUep9B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faSauiLUep9B",
        "outputId": "9d4fc2a6-07ba-4cee-dfd8-5ee3592e8d8e"
      },
      "outputs": [],
      "source": [
        "# %pip install swig\n",
        "# %pip install gym[all]\n",
        "# %pip install gymnasium\n",
        "# %pip install gymnasium[box2d]\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch.distributions as distributions\n",
        "from collections import deque\n",
        "import time\n",
        "\n",
        "# For visualization\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython.display import HTML\n",
        "from IPython import display\n",
        "import glob\n",
        "import base64, io\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import cv2\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import clear_output\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "Yt6FeSjPktcb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt6FeSjPktcb",
        "outputId": "c91cda9f-d738-4143-8a05-ada5842ef722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation space:  Box(0, 255, (96, 96, 3), uint8)\n",
            "Action space:  Discrete(5)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CarRacing-v2', continuous=False)\n",
        "print(\"Observation space: \", env.observation_space)\n",
        "print(\"Action space: \", env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ff04036c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Every frme always contains a black area at the bottom of the frame, so we had better cut this black area.\n",
        "# Also, Color imformation is not directly related to car racing. So we will use gray image for computation efficiency.\n",
        "# 학습에 불필요한 부분은 CROP 후에 사용함 (Grayscale로 변환)\n",
        "def preprocess(img):\n",
        "    img = img[:84, 6:90] # CarRacing-v2-specific cropping\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "02be91be",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImageEnv(gym.Wrapper):\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        skip_frames=4,\n",
        "        stack_frames=4,\n",
        "        initial_no_op=50,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(ImageEnv, self).__init__(env, **kwargs)\n",
        "        self.initial_no_op = initial_no_op\n",
        "        self.skip_frames = skip_frames\n",
        "        self.stack_frames = stack_frames\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the original environment.\n",
        "        s, info = self.env.reset()\n",
        "\n",
        "        # Do nothing for the next `self.initial_no_op` steps\n",
        "        for i in range(self.initial_no_op):\n",
        "            s, r, terminated, truncated, info = self.env.step(0)\n",
        "\n",
        "        # Convert a frame to 84 X 84 gray scale one\n",
        "        s = preprocess(s)\n",
        "\n",
        "        # The initial observation is simply a copy of the frame `s`\n",
        "        self.stacked_state = np.tile(s, (self.stack_frames, 1, 1))  # [4, 84, 84]\n",
        "        return self.stacked_state, info\n",
        "\n",
        "    def step(self, action):\n",
        "        # We take an action for self.skip_frames steps\n",
        "        # terminated: 완료 / truncated: 실패\n",
        "        reward = 0\n",
        "        for _ in range(self.skip_frames):\n",
        "            s, r, terminated, truncated, info = self.env.step(action)\n",
        "            reward += r\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        # Convert a frame to 84 X 84 gray scale one\n",
        "        s = preprocess(s)\n",
        "\n",
        "        # Push the current frame `s` at the end of self.stacked_state\n",
        "        self.stacked_state = np.concatenate((self.stacked_state[1:], s[np.newaxis]), axis=0)\n",
        "\n",
        "        return self.stacked_state, reward, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "8bae70f1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[[0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         ...,\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098]],\n",
              " \n",
              "        [[0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         ...,\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098]],\n",
              " \n",
              "        [[0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         ...,\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098]],\n",
              " \n",
              "        [[0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         ...,\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098],\n",
              "         [0.62745098, 0.62745098, 0.62745098, ..., 0.62745098,\n",
              "          0.62745098, 0.62745098]]]),\n",
              " {})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = gym.make('CarRacing-v2', continuous=False)\n",
        "env = ImageEnv(env)\n",
        "\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f3a3e3d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Actor, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(state_dim[0], 16, kernel_size=8, stride=4)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)\n",
        "    self.fc1 = nn.Linear(32 * 9 * 9, 128)\n",
        "    self.fc2 = nn.Linear(128, action_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.softmax(self.fc2(x), dim=-1)\n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self, state_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(state_dim[0], 16, kernel_size=8, stride=4)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)\n",
        "    self.fc1 = nn.Linear(32 * 9 * 9, 128)\n",
        "    self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ef388bb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
        "        return np.stack(state), action, reward, np.stack(next_state), done\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a8b21732",
      "metadata": {},
      "outputs": [],
      "source": [
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.1\n",
        "epsilon_decay = 500\n",
        "\n",
        "def get_epsilon(episode):\n",
        "    return max(epsilon_end, epsilon_start - (epsilon_start - epsilon_end) * (episode / epsilon_decay))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "699f1794",
      "metadata": {},
      "outputs": [],
      "source": [
        "state_dim = (4, 84, 84)\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "actor = Actor(state_dim, action_dim)\n",
        "critic= Critic(state_dim)\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr = 0.001)\n",
        "critic_optimizer=optim.Adam(critic.parameters(),lr=0.001)\n",
        "replay_buffer = ReplayBuffer(capacity=10000)\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "e70b5309",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(env, actor, critic, actor_optimizer, critic_optimizer, gamma, replay_buffer, batch_size, episode):\n",
        "    actor.train()\n",
        "    critic.train()\n",
        "    state, info = env.reset()\n",
        "    episode_reward = 0\n",
        "    terminated = truncated = False\n",
        "\n",
        "    while not terminated and not truncated:\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        action_probs = actor(state_tensor).detach().cpu().numpy().squeeze()\n",
        "\n",
        "        epsilon = get_epsilon(episode)\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.choice(action_dim)\n",
        "        else:\n",
        "            action = np.random.choice(np.arange(action_dim), p=action_probs)\n",
        "\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        replay_buffer.add(state, action, reward, next_state, terminated or truncated)\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        if replay_buffer.size() >= batch_size:\n",
        "            batch = replay_buffer.sample(batch_size)\n",
        "            state_batch, action_batch, reward_batch, next_state_batch, done_batch = batch\n",
        "\n",
        "            state_batch = torch.FloatTensor(state_batch)\n",
        "            next_state_batch = torch.FloatTensor(next_state_batch)\n",
        "            action_batch = torch.LongTensor(action_batch).view(-1, 1)\n",
        "            reward_batch = torch.FloatTensor(reward_batch).view(-1, 1)\n",
        "            done_batch = torch.FloatTensor(done_batch).view(-1, 1)\n",
        "\n",
        "            action_probs = actor(state_batch).gather(1, action_batch)\n",
        "            log_action_probs = torch.log(action_probs)\n",
        "\n",
        "            critic_t = critic(state_batch).view(-1, 1)\n",
        "            critic_td_t = reward_batch + (1 - done_batch) * gamma * critic(next_state_batch).view(-1, 1)\n",
        "            advantage_t = critic_td_t - critic_t\n",
        "\n",
        "            actor_loss = -torch.mean(advantage_t.detach() * log_action_probs)\n",
        "            actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            actor_optimizer.step()\n",
        "\n",
        "            critic_loss = F.smooth_l1_loss(critic_t, critic_td_t.detach())\n",
        "            critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            critic_optimizer.step()\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    return episode_reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "52863b1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "305e88f2",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/5000 [00:10<14:55:56, 10.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max Score Ever:  -48.27102803738362\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 2/5000 [00:22<15:41:57, 11.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max Score Ever:  -38.302180685358614\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 3/5000 [00:34<15:51:01, 11.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max Score Ever:  -36.22243281121819\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 3/5000 [00:35<16:31:10, 11.90s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[33], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, MAX_EPISODES \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m---> 11\u001b[0m     episode_reward \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(episode_reward)\n\u001b[1;32m     13\u001b[0m     scores_window\u001b[38;5;241m.\u001b[39mappend(episode_reward)\n",
            "Cell \u001b[0;32mIn[30], line 48\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, actor, critic, actor_optimizer, critic_optimizer, gamma, replay_buffer, batch_size, episode)\u001b[0m\n\u001b[1;32m     46\u001b[0m         critic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     47\u001b[0m         critic_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 48\u001b[0m         \u001b[43mcritic_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m episode_reward\n",
            "File \u001b[0;32m/data/wjkim9653/anaconda3/envs/ActorCritic/lib/python3.12/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m/data/wjkim9653/anaconda3/envs/ActorCritic/lib/python3.12/site-packages/torch/optim/optimizer.py:74\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m prev_grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Note on graph break below:\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# we need to graph break to ensure that aot respects the no_grad annotation.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# graph break to allow the fully fused fwd-bwd-optimizer graph to be compiled.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# see https://github.com/pytorch/pytorch/issues/104053\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m     76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/data/wjkim9653/anaconda3/envs/ActorCritic/lib/python3.12/site-packages/torch/autograd/grad_mode.py:186\u001b[0m, in \u001b[0;36mset_grad_enabled.__init__\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 186\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "MAX_EPISODES = 5000\n",
        "gamma = 0.99\n",
        "interval = 10\n",
        "scores = []\n",
        "scores_window = deque(maxlen=interval)\n",
        "maxscore = -10000\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for episode in tqdm(range(1, MAX_EPISODES + 1)):\n",
        "    episode_reward = train(env, actor, critic, actor_optimizer, critic_optimizer, gamma, replay_buffer, batch_size, episode)\n",
        "    scores.append(episode_reward)\n",
        "    scores_window.append(episode_reward)\n",
        "\n",
        "    avg_score = np.mean(scores_window)\n",
        "    if avg_score > maxscore:\n",
        "        maxscore = avg_score\n",
        "        print(\"Max Score Ever: \", avg_score)\n",
        "        torch.save(actor.state_dict(), 'actor.pt')\n",
        "        torch.save(critic.state_dict(), 'critic.pt')\n",
        "\n",
        "    if episode % interval == 0:\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, avg_score))\n",
        "\n",
        "    if avg_score >= 400.0:\n",
        "        print(\"END!!\")\n",
        "        print(\"It takes {} seconds\".format(time.time() - start))\n",
        "        torch.save(actor.state_dict(), 'checkpoint.pth')\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
